{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Cat Images from Stable Diffusion](images/stable-diffusion-images-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Stable Diffusion, a very popular foundation model, is a text-to-image generative AI model capable of creating photorealistic images given any text input within tens of seconds. Qualcomm AI Research performed full-stack AI optimizations using the Qualcomm AI Stack to deploy Stable Diffusion on an Android smartphone for the very first time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusion uses a latent diffusion model that iteratively applies denoising autoencoders. For a more computationally/memory efficient diffusion process, Stable Diffusion uses pretrained CLIP text encoder, U-Net, and variational autoencoder (VAE) models.\n",
    "The Stable Diffusion model takes a latent seed and a text prompt as input. The latent seed is used to generate random latent pixel image representations.\n",
    "* The text encoder transforms the text prompt to an embedding space that the U-Net can understand.\n",
    "* The VAE compresses the image from pixel space to a smaller dimensional latent space to input to the U-Net model.\n",
    "* U-Net iteratively denoises the random latent image representations while conditioning on the text embeddings to improve latent image representations that U-Net outputs.\n",
    "* The VAE decoder generates the final image by translating the representation back into its original pixel space form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows how stable diffusion works during inference\n",
    "\n",
    "![Stable Diffusion inference steps](images/stable_diffusion_architecture.png)\n",
    "\n",
    "The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to generate random latent image representations of size 64×64 where as the text prompt is transformed to text embeddings of size\n",
    "77×768 via CLIP's text encoder.\n",
    "\n",
    "Next the U-Net iteratively denoises the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. The denoising process is repeated ca. 50 times to step-by-step retrieve better latent image representations. Once complete, the latent image representation is decoded by the decoder part of the variational auto encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "### Platform requirements\n",
    "This notebook is intended to run on a machine with\n",
    "- Machine running Ubuntu 20.04\n",
    "- NVIDIA driver version equivalent to 525.60.13\n",
    "\n",
    "### Installing package dependencies\n",
    "\n",
    "1. Ensure that you have installed docker and nvidia docker2 runtime: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#docker.\n",
    "2. Navigate to the folder containing this jupyter notebook and launch the container. See https://docs.qualcomm.com/bundle/publicresource/topics/80-64748-1/introduction.html for instructions on how to download this notebook.\n",
    "\n",
    "    `docker run --rm --gpus all --name=aimet-dev-torch-gpu -v $PWD:$PWD -w $PWD -v /etc/localtime:/etc/localtime:ro -v /etc/timezone:/etc/timezone:ro --network=host --ulimit core=-1 --ipc=host --shm-size=8G --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -it artifacts.codelinaro.org/codelinaro-aimet/aimet-dev:latest.torch-gpu`\n",
    "\n",
    "    Alternatively, you can download and build AIMET docker using this Dockerfile: https://github.com/quic/aimet/blob/develop/Jenkins/Dockerfile.torch-gpu.\n",
    "\n",
    "4. To install additional dependencies and start the jupyter server run the launch.sh script.\n",
    "`./launch.sh`\n",
    "\n",
    "5. Once the server has started you will be presented URL's to copy and paste into your browser.\n",
    "\n",
    "6. From the jupyter home page select the stable_diffusion.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pre-trained FP32 models\n",
    "\n",
    "Note that when this is run for the first time, these large model checkpoints (5-6GB) will be downloaded to the current folder on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T11:28:32.499619Z",
     "start_time": "2023-05-17T11:28:30.162966Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from redefined_modules.transformers.models.clip.modeling_clip import CLIPTextModel\n",
    "from redefined_modules.diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
    "from redefined_modules.diffusers.models.vae import AutoencoderKLDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T11:29:06.094704Z",
     "start_time": "2023-05-17T11:28:32.515323Z"
    }
   },
   "outputs": [],
   "source": [
    "cache_dir = \"./_data_/cache/huggingface/diffusers\"\n",
    "device = 'cuda'\n",
    "dtype = torch.float\n",
    "\n",
    "print(\"Loading pre-trained TextEncoder model\")\n",
    "text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14',\n",
    "                                             torch_dtype=dtype, cache_dir=cache_dir).to(device)\n",
    "text_encoder.config.return_dict = False\n",
    "\n",
    "print(\"Loading pre-trained UNET model\")\n",
    "unet = UNet2DConditionModel.from_pretrained('runwayml/stable-diffusion-v1-5',\n",
    "                                            subfolder=\"unet\", revision='main', torch_dtype=dtype,\n",
    "                                            cache_dir=cache_dir).to(device)\n",
    "unet.config.return_dict = False\n",
    "\n",
    "print(\"Loading pre-trained VAE model\")\n",
    "vae = AutoencoderKLDecoder.from_pretrained('runwayml/stable-diffusion-v1-5',\n",
    "                                           revision='main', subfolder=\"vae\", torch_dtype=dtype,\n",
    "                                           cache_dir=cache_dir).to(device)\n",
    "vae.config.return_dict = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, unroll the multi-head attention (MHA) blocks in the model to individual single-head attention (SHA) blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T11:29:06.746298Z",
     "start_time": "2023-05-17T11:29:06.093735Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_diff_pipeline import run_the_pipeline, save_image, replace_mha_with_sha_blocks\n",
    "replace_mha_with_sha_blocks(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a floating-point evaluation\n",
    "Run an example prompt through the loaded FP32 model and check the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T11:29:15.154798Z",
     "start_time": "2023-05-17T11:29:06.746298Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "from stable_diff_pipeline import run_tokenizer, run_text_encoder, run_diffusion_steps, run_vae_decoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14', cache_dir=cache_dir)\n",
    "\n",
    "prompt = \"decorated modern country house interior, 8 k, light reflections\"\n",
    "image = run_the_pipeline(prompt, unet, text_encoder, vae, tokenizer, test_name='fp32', seed=1.36477711e+14)\n",
    "save_image(image.squeeze(0), 'generated.png')\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename='generated.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example pre-quantization optimization result**\n",
    "\n",
    "![Example result; decorated country house living room interior](./images/example_pre_quant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply AIMET quantization optimization techniques\n",
    "Use the AI Model Efficiency Toolkit (AIMET) to create quantization simulation models (QuantSim) for the text encoder, U-Net, and VAE using a mixed-precision quantization scheme. A calibration process is used to create these QuantSim models where per-layer quantization encodings are determined using representative data samples. The QuantSim models simulate running the stable diffusion models on a quantized target. In addition, for the Text Encoder model, the AIMET Adaptive Rounding (AdaRound) technique is applied to get a boost in quantized accuracy.\n",
    "\n",
    "Set the parameter bitwidth to INT8 and the activation bitwidth to INT16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply AIMET Adaptive Rounding (AdaRound) to the TE model\n",
    "To help recover quantized accuracy of the TE model, apply the AIMET AdaRound optimization technique. AdaRound performs layer-by-layer optimization to learn a rounding matrix of how the layer weights get rounded.\n",
    "\n",
    "Expected execution time: 19min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T11:48:22.841599Z",
     "start_time": "2023-05-17T11:29:15.273321Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from argparse import Namespace\n",
    "from aimet_quantsim import apply_adaround_te, calibrate_te\n",
    "\n",
    "with open('config.json', 'rt') as f:\n",
    "    config = Namespace(**json.load(f))\n",
    "\n",
    "with open(config.calibration_prompts, \"rt\") as f:\n",
    "    print(f'Loading prompts from {config.calibration_prompts}')\n",
    "    prompts = f.readlines()\n",
    "    prompts = prompts[:50]\n",
    "\n",
    "tokens = [run_tokenizer(tokenizer, prompt) for prompt in prompts]\n",
    "\n",
    "text_encoder_sim = apply_adaround_te(text_encoder, tokens, config)\n",
    "del text_encoder\n",
    "text_encoder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate the TE model\n",
    "We use 20 prompts to calibrate the TE Quantization Sim (QuantSim) Model. Using more prompts may help increase the quantized accuracy, while taking proportionally more time for the calibration process itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T11:48:29.730293Z",
     "start_time": "2023-05-17T11:48:22.825484Z"
    }
   },
   "outputs": [],
   "source": [
    "text_encoder_sim = calibrate_te(text_encoder_sim, tokens, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate the UNET model\n",
    "Next create a QuantSim model and calibrate the UNET.\n",
    "Use the same prompts used for calibrating the TE model to generate embeddings that are used to calibrate the UNET model. The prompts are fed as inputs to the already-calibrated TE QuantSim model and the resulting embeddings are stored to use as calibration data for the UNET.\n",
    "\n",
    "Expected execution time: 28min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T12:17:22.452558Z",
     "start_time": "2023-05-17T11:48:29.730293Z"
    }
   },
   "outputs": [],
   "source": [
    "from aimet_quantsim import calibrate_unet\n",
    "\n",
    "embeddings = [(run_text_encoder(text_encoder_sim.model, uncond),\n",
    "               run_text_encoder(text_encoder_sim.model, cond)) for cond, uncond in tokens]\n",
    "embeddings = [torch.cat([uncond, cond])for uncond, cond in embeddings]\n",
    "\n",
    "unet_sim = calibrate_unet(unet, embeddings, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate the VAE model\n",
    "Next create a QuantSim model and calibrate the VAE.\n",
    "The embeddings used in the previous step are fed as inputs to the already-calibrated UNET QuantSim model and the resulting latents are stored to use as calibration data for the VAE.\n",
    "\n",
    "Expected execution time: 18min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T12:35:48.684463Z",
     "start_time": "2023-05-17T12:17:22.452558Z"
    }
   },
   "outputs": [],
   "source": [
    "from aimet_quantsim import calibrate_vae\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "latents = [run_diffusion_steps(unet_sim.model, i) for i in tqdm(embeddings)]\n",
    "print('Obtained latents using UNET QuantSim')\n",
    "\n",
    "vae_sim = calibrate_vae(vae, latents, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Quantized off-target Inference\n",
    "\n",
    "With AIMET QuantSim versions of the text encoder, U-Net, and VAE models, the same pipeline can be used to run a simulated quantized inference of the Stable Diffusion model. Pass in the sim versions of the models. The rest of the pipeline is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T12:36:11.346206Z",
     "start_time": "2023-05-17T12:35:48.684463Z"
    }
   },
   "outputs": [],
   "source": [
    "image = run_the_pipeline(prompt, unet_sim.model, text_encoder_sim.model, vae_sim.model, tokenizer, test_name=\"int8\", seed=1.36477711e+14)\n",
    "save_image(image.squeeze(0), 'generated_after_quant.png')\n",
    "\n",
    "display(Image(filename='generated_after_quant.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example post-quantization optimization result**\n",
    "\n",
    "![Example post-quantization optimization result; decorated living room interior](images/example_post_quant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model\n",
    "Model optimization using AIMET is complete. Export the model and the corresponding quantization encodings.\n",
    "\n",
    "Expected execution time: 18m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T12:54:25.756766Z",
     "start_time": "2023-05-17T12:36:11.346206Z"
    }
   },
   "outputs": [],
   "source": [
    "from aimet_quantsim import export_all_models\n",
    "\n",
    "export_all_models(text_encoder_sim, unet_sim, vae_sim, tokens, embeddings, latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Qualcomm Technologies, Inc. and/or its subsidiaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
